<!DOCTYPE html>

<html lang="en">

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <meta name="mobile-web-app-capable" content="yes">
    <title>
        Human Cognition Inspired Word Segmentation Models - HackMD
    </title>
    <link rel="icon" type="image/png" href="https://hackmd.io/favicon.png">
    <link rel="apple-touch-icon" href="https://hackmd.io/apple-touch-icon.png">

    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha256-916EbMg70RQy9LHiGkXzG8hSg9EdNy97GazNG/aiY1w=" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha256-eZrrJcwDc/3uDhsdt61sL2oOBY362qM3lon1gyExkL0=" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/ionicons/2.0.1/css/ionicons.min.css" integrity="sha256-3iu9jgsy9TpTwXKb7bNQzqWekRX7pPK+2OLj3R922fo=" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/octicons/3.5.0/octicons.min.css" integrity="sha256-QiWfLIsCT02Sdwkogf6YMiQlj4NE84MKkzEMkZnMGdg=" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.5.1/themes/prism.min.css" integrity="sha256-vtR0hSWRc3Tb26iuN2oZHt3KRUomwTufNIf5/4oeCyg=" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@hackmd/emojify.js@2.1.0/dist/css/basic/emojify.min.css" integrity="sha256-UOrvMOsSDSrW6szVLe8ZDZezBxh5IoIfgTwdNDgTjiU=" crossorigin="anonymous" />
    <style>
        @import url(https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,500,500i|Source+Code+Pro:300,400,500|Source+Sans+Pro:300,300i,400,400i,600,600i|Source+Serif+Pro&subset=latin-ext);.hljs{display:block;background:#fff;padding:.5em;color:#333;overflow-x:auto}.hljs-comment,.hljs-meta{color:#969896}.hljs-emphasis,.hljs-quote,.hljs-string,.hljs-strong,.hljs-template-variable,.hljs-variable{color:#df5000}.hljs-keyword,.hljs-selector-tag,.hljs-type{color:#a71d5d}.hljs-attribute,.hljs-bullet,.hljs-literal,.hljs-number,.hljs-symbol{color:#0086b3}.hljs-built_in,.hljs-builtin-name{color:#005cc5}.hljs-name,.hljs-section{color:#63a35c}.hljs-tag{color:#333}.hljs-attr,.hljs-selector-attr,.hljs-selector-class,.hljs-selector-id,.hljs-selector-pseudo,.hljs-title{color:#795da3}.hljs-addition{color:#55a532;background-color:#eaffea}.hljs-deletion{color:#bd2c00;background-color:#ffecec}.hljs-link{text-decoration:underline}.markdown-body{font-size:16px;line-height:1.5;word-wrap:break-word}.markdown-body:after,.markdown-body:before{display:table;content:""}.markdown-body:after{clear:both}.markdown-body>:first-child{margin-top:0!important}.markdown-body>:last-child{margin-bottom:0!important}.markdown-body a:not([href]){color:inherit;text-decoration:none}.markdown-body .absent{color:#c00}.markdown-body .anchor{float:left;padding-right:4px;margin-left:-20px;line-height:1}.markdown-body .anchor:focus{outline:none}.markdown-body blockquote,.markdown-body dl,.markdown-body ol,.markdown-body p,.markdown-body pre,.markdown-body table,.markdown-body ul{margin-top:0;margin-bottom:16px}.markdown-body hr{height:.25em;padding:0;margin:24px 0;background-color:#e7e7e7;border:0}.markdown-body blockquote{font-size:16px;padding:0 1em;color:#777;border-left:.25em solid #ddd}.markdown-body blockquote>:first-child{margin-top:0}.markdown-body blockquote>:last-child{margin-bottom:0}.markdown-body kbd,.popover kbd{display:inline-block;padding:3px 5px;font-size:11px;line-height:10px;color:#555;vertical-align:middle;background-color:#fcfcfc;border:1px solid #ccc;border-bottom-color:#bbb;border-radius:3px;box-shadow:inset 0 -1px 0 #bbb}.markdown-body .loweralpha{list-style-type:lower-alpha}.markdown-body h1,.markdown-body h2,.markdown-body h3,.markdown-body h4,.markdown-body h5,.markdown-body h6{margin-top:24px;margin-bottom:16px;font-weight:600;line-height:1.25}.markdown-body h1 .octicon-link,.markdown-body h2 .octicon-link,.markdown-body h3 .octicon-link,.markdown-body h4 .octicon-link,.markdown-body h5 .octicon-link,.markdown-body h6 .octicon-link{color:#000;vertical-align:middle;visibility:hidden}.markdown-body h1:hover .anchor,.markdown-body h2:hover .anchor,.markdown-body h3:hover .anchor,.markdown-body h4:hover .anchor,.markdown-body h5:hover .anchor,.markdown-body h6:hover .anchor{text-decoration:none}.markdown-body h1:hover .anchor .octicon-link,.markdown-body h2:hover .anchor .octicon-link,.markdown-body h3:hover .anchor .octicon-link,.markdown-body h4:hover .anchor .octicon-link,.markdown-body h5:hover .anchor .octicon-link,.markdown-body h6:hover .anchor .octicon-link{visibility:visible}.markdown-body h1 code,.markdown-body h1 tt,.markdown-body h2 code,.markdown-body h2 tt,.markdown-body h3 code,.markdown-body h3 tt,.markdown-body h4 code,.markdown-body h4 tt,.markdown-body h5 code,.markdown-body h5 tt,.markdown-body h6 code,.markdown-body h6 tt{font-size:inherit}.markdown-body h1{font-size:2em}.markdown-body h1,.markdown-body h2{padding-bottom:.3em;border-bottom:1px solid #eee}.markdown-body h2{font-size:1.5em}.markdown-body h3{font-size:1.25em}.markdown-body h4{font-size:1em}.markdown-body h5{font-size:.875em}.markdown-body h6{font-size:.85em;color:#777}.markdown-body ol,.markdown-body ul{padding-left:2em}.markdown-body ol.no-list,.markdown-body ul.no-list{padding:0;list-style-type:none}.markdown-body ol ol,.markdown-body ol ul,.markdown-body ul ol,.markdown-body ul ul{margin-top:0;margin-bottom:0}.markdown-body li>p{margin-top:16px}.markdown-body li+li{padding-top:.25em}.markdown-body dl{padding:0}.markdown-body dl dt{padding:0;margin-top:16px;font-size:1em;font-style:italic;font-weight:700}.markdown-body dl dd{padding:0 16px;margin-bottom:16px}.markdown-body table{display:block;width:100%;overflow:auto;word-break:normal;word-break:keep-all}.markdown-body table th{font-weight:700}.markdown-body table td,.markdown-body table th{padding:6px 13px;border:1px solid #ddd}.markdown-body table tr{background-color:#fff;border-top:1px solid #ccc}.markdown-body table tr:nth-child(2n){background-color:#f8f8f8}.markdown-body img{max-width:100%;box-sizing:content-box;background-color:#fff}.markdown-body img[align=right]{padding-left:20px}.markdown-body img[align=left]{padding-right:20px}.markdown-body .emoji{max-width:none;vertical-align:text-top;background-color:transparent}.markdown-body span.frame{display:block;overflow:hidden}.markdown-body span.frame>span{display:block;float:left;width:auto;padding:7px;margin:13px 0 0;overflow:hidden;border:1px solid #ddd}.markdown-body span.frame span img{display:block;float:left}.markdown-body span.frame span span{display:block;padding:5px 0 0;clear:both;color:#333}.markdown-body span.align-center{display:block;overflow:hidden;clear:both}.markdown-body span.align-center>span{display:block;margin:13px auto 0;overflow:hidden;text-align:center}.markdown-body span.align-center span img{margin:0 auto;text-align:center}.markdown-body span.align-right{display:block;overflow:hidden;clear:both}.markdown-body span.align-right>span{display:block;margin:13px 0 0;overflow:hidden;text-align:right}.markdown-body span.align-right span img{margin:0;text-align:right}.markdown-body span.float-left{display:block;float:left;margin-right:13px;overflow:hidden}.markdown-body span.float-left span{margin:13px 0 0}.markdown-body span.float-right{display:block;float:right;margin-left:13px;overflow:hidden}.markdown-body span.float-right>span{display:block;margin:13px auto 0;overflow:hidden;text-align:right}.markdown-body code,.markdown-body tt{padding:0;padding-top:.2em;padding-bottom:.2em;margin:0;font-size:85%;background-color:rgba(0,0,0,.04);border-radius:3px}.markdown-body code:after,.markdown-body code:before,.markdown-body tt:after,.markdown-body tt:before{letter-spacing:-.2em;content:"\00a0"}.markdown-body code br,.markdown-body tt br{display:none}.markdown-body del code{text-decoration:inherit}.markdown-body pre{word-wrap:normal}.markdown-body pre>code{padding:0;margin:0;font-size:100%;word-break:normal;white-space:pre;background:transparent;border:0}.markdown-body .highlight{margin-bottom:16px}.markdown-body .highlight pre{margin-bottom:0;word-break:normal}.markdown-body .highlight pre,.markdown-body pre{padding:16px;overflow:auto;font-size:85%;line-height:1.45;background-color:#f7f7f7;border-radius:3px}.markdown-body pre code,.markdown-body pre tt{display:inline;max-width:auto;padding:0;margin:0;overflow:visible;line-height:inherit;word-wrap:normal;background-color:transparent;border:0}.markdown-body pre code:after,.markdown-body pre code:before,.markdown-body pre tt:after,.markdown-body pre tt:before{content:normal}.markdown-body .csv-data td,.markdown-body .csv-data th{padding:5px;overflow:hidden;font-size:12px;line-height:1;text-align:left;white-space:nowrap}.markdown-body .csv-data .blob-line-num{padding:10px 8px 9px;text-align:right;background:#fff;border:0}.markdown-body .csv-data tr{border-top:0}.markdown-body .csv-data th{font-weight:700;background:#f8f8f8;border-top:0}.news .alert .markdown-body blockquote{padding:0 0 0 40px;border:0 none}.activity-tab .news .alert .commits,.activity-tab .news .markdown-body blockquote{padding-left:0}.task-list-item{list-style-type:none}.task-list-item label{font-weight:400}.task-list-item.enabled label{cursor:pointer}.task-list-item+.task-list-item{margin-top:3px}.task-list-item-checkbox{float:left;margin:.31em 0 .2em -1.3em!important;vertical-align:middle;cursor:default!important}.markdown-body{padding-top:40px;padding-bottom:40px;max-width:758px;overflow:visible!important;position:relative}.markdown-body .emoji{vertical-align:top}.markdown-body pre{border:inherit!important}.markdown-body code{color:inherit!important}.markdown-body pre code .wrapper{display:-moz-inline-flex;display:-ms-inline-flex;display:-o-inline-flex;display:inline-flex}.markdown-body pre code .gutter{float:left;overflow:hidden;-webkit-user-select:none;user-select:none}.markdown-body pre code .gutter.linenumber{text-align:right;position:relative;display:inline-block;cursor:default;z-index:4;padding:0 8px 0 0;min-width:20px;box-sizing:content-box;color:#afafaf!important;border-right:3px solid #6ce26c!important}.markdown-body pre code .gutter.linenumber>span:before{content:attr(data-linenumber)}.markdown-body pre code .code{float:left;margin:0 0 0 16px}.markdown-body .gist .line-numbers{border-left:none;border-top:none;border-bottom:none}.markdown-body .gist .line-data{border:none}.markdown-body .gist table{border-spacing:0;border-collapse:inherit!important}.markdown-body code[data-gist-id]{background:none;padding:0}.markdown-body code[data-gist-id]:after,.markdown-body code[data-gist-id]:before{content:""}.markdown-body code[data-gist-id] .blob-num{border:unset}.markdown-body code[data-gist-id] table{overflow:unset;margin-bottom:unset}.markdown-body code[data-gist-id] table tr{background:unset}.markdown-body[dir=rtl] pre{direction:ltr}.markdown-body[dir=rtl] code{direction:ltr;unicode-bidi:embed}.markdown-body .alert>p:last-child{margin-bottom:0}.markdown-body pre.abc,.markdown-body pre.flow-chart,.markdown-body pre.graphviz,.markdown-body pre.mermaid,.markdown-body pre.sequence-diagram,.markdown-body pre.vega{text-align:center;background-color:inherit;border-radius:0;white-space:inherit;overflow:visible}.markdown-body pre.abc>code,.markdown-body pre.flow-chart>code,.markdown-body pre.graphviz>code,.markdown-body pre.mermaid>code,.markdown-body pre.sequence-diagram>code,.markdown-body pre.vega>code{text-align:left}.markdown-body pre.abc>svg,.markdown-body pre.flow-chart>svg,.markdown-body pre.graphviz>svg,.markdown-body pre.mermaid>svg,.markdown-body pre.sequence-diagram>svg,.markdown-body pre.vega>svg{max-width:100%;height:100%}.markdown-body pre>code.wrap{white-space:pre-wrap;white-space:-moz-pre-wrap;white-space:-pre-wrap;white-space:-o-pre-wrap;word-wrap:break-word}.markdown-body .alert>p:last-child,.markdown-body .alert>ul:last-child{margin-bottom:0}.markdown-body summary{display:list-item}.markdown-body summary:focus{outline:none}.markdown-body details summary{cursor:pointer}.markdown-body details:not([open])>:not(summary){display:none}.markdown-body figure{margin:1em 40px}.markdown-body .mark,.markdown-body mark{background-color:#fff1a7}.vimeo,.youtube{cursor:pointer;display:table;text-align:center;background-position:50%;background-repeat:no-repeat;background-size:contain;background-color:#000;overflow:hidden}.vimeo,.youtube{position:relative;width:100%}.youtube{padding-bottom:56.25%}.vimeo img{width:100%;object-fit:contain;z-index:0}.youtube img{object-fit:cover;z-index:0}.vimeo iframe,.youtube iframe,.youtube img{width:100%;height:100%;position:absolute;top:0;left:0}.vimeo iframe,.youtube iframe{vertical-align:middle;z-index:1}.vimeo .icon,.youtube .icon{position:absolute;height:auto;width:auto;top:50%;left:50%;transform:translate(-50%,-50%);color:#fff;opacity:.3;transition:opacity .2s;z-index:0}.vimeo:hover .icon,.youtube:hover .icon{opacity:.6;transition:opacity .2s}.slideshare .inner,.speakerdeck .inner{position:relative;width:100%}.slideshare .inner iframe,.speakerdeck .inner iframe{position:absolute;top:0;bottom:0;left:0;right:0;width:100%;height:100%}.figma{display:table;position:relative;width:100%;padding-bottom:56.25%}.figma iframe{position:absolute;top:0;bottom:0;left:0;right:0;width:100%;height:100%;border:1px solid #eee}.MJX_Assistive_MathML{display:none}#MathJax_Message{z-index:1000!important}.ui-infobar{position:relative;z-index:2;max-width:760px;margin:25px auto -25px;color:#777}.toc .invisable-node{list-style-type:none}.ui-toc{position:fixed;bottom:20px;z-index:998}.ui-toc.both-mode{margin-left:8px}.ui-toc.both-mode .ui-toc-label{height:40px;padding:10px 4px;border-top-left-radius:0;border-bottom-left-radius:0}.ui-toc-label{background-color:#e6e6e6;border:none;color:#868686;transition:opacity .2s}.ui-toc .open .ui-toc-label{opacity:1;color:#fff;transition:opacity .2s}.ui-toc-label:focus{opacity:.3;background-color:#ccc;color:#000}.ui-toc-label:hover{opacity:1;background-color:#ccc;transition:opacity .2s}.ui-toc-dropdown{margin-top:20px;margin-bottom:20px;padding-left:10px;padding-right:10px;max-width:45vw;width:25vw;max-height:70vh;overflow:auto;text-align:inherit}.ui-toc-dropdown>.toc{max-height:calc(70vh - 100px);overflow:auto}.ui-toc-dropdown[dir=rtl] .nav{padding-right:0;letter-spacing:.0029em}.ui-toc-dropdown a{overflow:hidden;text-overflow:ellipsis;white-space:pre}.ui-toc-dropdown .nav>li>a{display:block;padding:4px 20px;font-size:13px;font-weight:500;color:#767676}.ui-toc-dropdown .nav>li:first-child:last-child>ul,.ui-toc-dropdown .toc.expand ul{display:block}.ui-toc-dropdown .nav>li>a:focus,.ui-toc-dropdown .nav>li>a:hover{padding-left:19px;color:#000;text-decoration:none;background-color:transparent;border-left:1px solid #000}.ui-toc-dropdown[dir=rtl] .nav>li>a:focus,.ui-toc-dropdown[dir=rtl] .nav>li>a:hover{padding-right:19px;border-left:none;border-right:1px solid #000}.ui-toc-dropdown .nav>.active:focus>a,.ui-toc-dropdown .nav>.active:hover>a,.ui-toc-dropdown .nav>.active>a{padding-left:18px;font-weight:700;color:#000;background-color:transparent;border-left:2px solid #000}.ui-toc-dropdown[dir=rtl] .nav>.active:focus>a,.ui-toc-dropdown[dir=rtl] .nav>.active:hover>a,.ui-toc-dropdown[dir=rtl] .nav>.active>a{padding-right:18px;border-left:none;border-right:2px solid #000}.ui-toc-dropdown .nav .nav{display:none;padding-bottom:10px}.ui-toc-dropdown .nav>.active>ul{display:block}.ui-toc-dropdown .nav .nav>li>a{padding-top:1px;padding-bottom:1px;padding-left:30px;font-size:12px;font-weight:400}.ui-toc-dropdown[dir=rtl] .nav .nav>li>a{padding-right:30px}.ui-toc-dropdown .nav .nav>li>ul>li>a{padding-top:1px;padding-bottom:1px;padding-left:40px;font-size:12px;font-weight:400}.ui-toc-dropdown[dir=rtl] .nav .nav>li>ul>li>a{padding-right:40px}.ui-toc-dropdown .nav .nav>li>a:focus,.ui-toc-dropdown .nav .nav>li>a:hover{padding-left:29px}.ui-toc-dropdown[dir=rtl] .nav .nav>li>a:focus,.ui-toc-dropdown[dir=rtl] .nav .nav>li>a:hover{padding-right:29px}.ui-toc-dropdown .nav .nav>li>ul>li>a:focus,.ui-toc-dropdown .nav .nav>li>ul>li>a:hover{padding-left:39px}.ui-toc-dropdown[dir=rtl] .nav .nav>li>ul>li>a:focus,.ui-toc-dropdown[dir=rtl] .nav .nav>li>ul>li>a:hover{padding-right:39px}.ui-toc-dropdown .nav .nav>.active:focus>a,.ui-toc-dropdown .nav .nav>.active:hover>a,.ui-toc-dropdown .nav .nav>.active>a{padding-left:28px;font-weight:500}.ui-toc-dropdown[dir=rtl] .nav .nav>.active:focus>a,.ui-toc-dropdown[dir=rtl] .nav .nav>.active:hover>a,.ui-toc-dropdown[dir=rtl] .nav .nav>.active>a{padding-right:28px}.ui-toc-dropdown .nav .nav>.active>.nav>.active:focus>a,.ui-toc-dropdown .nav .nav>.active>.nav>.active:hover>a,.ui-toc-dropdown .nav .nav>.active>.nav>.active>a{padding-left:38px;font-weight:500}.ui-toc-dropdown[dir=rtl] .nav .nav>.active>.nav>.active:focus>a,.ui-toc-dropdown[dir=rtl] .nav .nav>.active>.nav>.active:hover>a,.ui-toc-dropdown[dir=rtl] .nav .nav>.active>.nav>.active>a{padding-right:38px}.markdown-body{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Helvetica Neue,Helvetica,Roboto,Arial,sans-serif}html[lang^=ja] .markdown-body{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Helvetica Neue,Helvetica,Roboto,Arial,Hiragino Kaku Gothic Pro,ヒラギノ角ゴ Pro W3,Osaka,Meiryo,メイリオ,MS Gothic,ＭＳ\ ゴシック,sans-serif}html[lang=zh-tw] .markdown-body{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Helvetica Neue,Helvetica,Roboto,Arial,PingFang TC,Microsoft JhengHei,微軟正黑,sans-serif}html[lang=zh-cn] .markdown-body{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Helvetica Neue,Helvetica,Roboto,Arial,PingFang SC,Microsoft YaHei,微软雅黑,sans-serif}html .markdown-body[lang^=ja]{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Helvetica Neue,Helvetica,Roboto,Arial,Hiragino Kaku Gothic Pro,ヒラギノ角ゴ Pro W3,Osaka,Meiryo,メイリオ,MS Gothic,ＭＳ\ ゴシック,sans-serif}html .markdown-body[lang=zh-tw]{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Helvetica Neue,Helvetica,Roboto,Arial,PingFang TC,Microsoft JhengHei,微軟正黑,sans-serif}html .markdown-body[lang=zh-cn]{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Helvetica Neue,Helvetica,Roboto,Arial,PingFang SC,Microsoft YaHei,微软雅黑,sans-serif}html[lang^=ja] .ui-toc-dropdown{font-family:Source Sans Pro,Helvetica,Arial,Meiryo UI,MS PGothic,ＭＳ\ Ｐゴシック,sans-serif}html[lang=zh-tw] .ui-toc-dropdown{font-family:Source Sans Pro,Helvetica,Arial,Microsoft JhengHei UI,微軟正黑UI,sans-serif}html[lang=zh-cn] .ui-toc-dropdown{font-family:Source Sans Pro,Helvetica,Arial,Microsoft YaHei UI,微软雅黑UI,sans-serif}html .ui-toc-dropdown[lang^=ja]{font-family:Source Sans Pro,Helvetica,Arial,Meiryo UI,MS PGothic,ＭＳ\ Ｐゴシック,sans-serif}html .ui-toc-dropdown[lang=zh-tw]{font-family:Source Sans Pro,Helvetica,Arial,Microsoft JhengHei UI,微軟正黑UI,sans-serif}html .ui-toc-dropdown[lang=zh-cn]{font-family:Source Sans Pro,Helvetica,Arial,Microsoft YaHei UI,微软雅黑UI,sans-serif}.ui-affix-toc{position:fixed;top:0;max-width:15vw;max-height:70vh;overflow:auto}.back-to-top,.expand-toggle,.go-to-bottom{display:block;padding:4px 10px;margin-top:10px;margin-left:10px;font-size:12px;font-weight:500;color:#999}.back-to-top:focus,.back-to-top:hover,.expand-toggle:focus,.expand-toggle:hover,.go-to-bottom:focus,.go-to-bottom:hover{color:#563d7c;text-decoration:none}.back-to-top,.go-to-bottom{margin-top:0}.ui-user-icon{width:20px;height:20px;display:block;border-radius:50%;margin-top:2px;margin-bottom:2px;margin-right:5px;background-position:50%;background-repeat:no-repeat;background-size:cover}.ui-user-icon.small{width:18px;height:18px;display:inline-block;vertical-align:middle;margin:0 0 .2em}.ui-infobar>small>span{line-height:22px}.ui-infobar>small .dropdown{display:inline-block}.ui-infobar>small .dropdown a:focus,.ui-infobar>small .dropdown a:hover{text-decoration:none}.ui-more-info{color:#888;cursor:pointer;vertical-align:middle}.ui-more-info .fa{font-size:16px}.ui-connectedGithub,.ui-published-note{color:#888}.ui-connectedGithub{line-height:23px;white-space:nowrap}.ui-connectedGithub a.file-path{color:#888;text-decoration:none;padding-left:22px}.ui-connectedGithub a.file-path:active,.ui-connectedGithub a.file-path:hover{color:#888;text-decoration:underline}.ui-connectedGithub .fa{font-size:20px}.ui-published-note .fa{font-size:20px;vertical-align:top}.unselectable{-webkit-user-select:none;-o-user-select:none;user-select:none}.selectable{-webkit-user-select:text;-o-user-select:text;user-select:text}@media print{blockquote,div,img,pre,table{page-break-inside:avoid!important}a[href]:after{font-size:12px!important}}.markdown-body.slides{position:relative;z-index:1;color:#222}.markdown-body.slides:before{content:"";display:block;position:absolute;top:0;left:0;right:0;bottom:0;z-index:-1;background-color:currentColor;box-shadow:0 0 0 50vw}.markdown-body.slides section[data-markdown]{position:relative;margin-bottom:1.5em;background-color:#fff;text-align:center}.markdown-body.slides section[data-markdown] code{text-align:left}.markdown-body.slides section[data-markdown]:before{content:"";display:block;padding-bottom:56.23%}.markdown-body.slides section[data-markdown]>div:first-child{position:absolute;top:50%;left:1em;right:1em;transform:translateY(-50%);max-height:100%;overflow:hidden}.markdown-body.slides section[data-markdown]>ul{display:inline-block}.markdown-body.slides>section>section+section:after{content:"";position:absolute;top:-1.5em;right:1em;height:1.5em;border:3px solid #777}.site-ui-font{font-family:Source Sans Pro,Helvetica,Arial,sans-serif}html[lang^=ja] .site-ui-font{font-family:Source Sans Pro,Helvetica,Arial,Hiragino Kaku Gothic Pro,ヒラギノ角ゴ Pro W3,Osaka,Meiryo,メイリオ,MS Gothic,ＭＳ\ ゴシック,sans-serif}html[lang=zh-tw] .site-ui-font{font-family:Source Sans Pro,Helvetica,Arial,PingFang TC,Microsoft JhengHei,微軟正黑,sans-serif}html[lang=zh-cn] .site-ui-font{font-family:Source Sans Pro,Helvetica,Arial,PingFang SC,Microsoft YaHei,微软雅黑,sans-serif}body{font-smoothing:subpixel-antialiased!important;-webkit-font-smoothing:subpixel-antialiased!important;-moz-osx-font-smoothing:auto!important;text-shadow:0 0 1em transparent,1px 1px 1.2px rgba(0,0,0,.004);-webkit-overflow-scrolling:touch;letter-spacing:.025em;font-family:Source Sans Pro,Helvetica,Arial,sans-serif}html[lang^=ja] body{font-family:Source Sans Pro,Helvetica,Arial,Hiragino Kaku Gothic Pro,ヒラギノ角ゴ Pro W3,Osaka,Meiryo,メイリオ,MS Gothic,ＭＳ\ ゴシック,sans-serif}html[lang=zh-tw] body{font-family:Source Sans Pro,Helvetica,Arial,PingFang TC,Microsoft JhengHei,微軟正黑,sans-serif}html[lang=zh-cn] body{font-family:Source Sans Pro,Helvetica,Arial,PingFang SC,Microsoft YaHei,微软雅黑,sans-serif}abbr[title]{border-bottom:none;text-decoration:underline;-webkit-text-decoration:underline dotted;text-decoration:underline dotted}abbr[data-original-title],abbr[title]{cursor:help}body.modal-open{overflow-y:auto;padding-right:0!important}
    </style>
    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
    	<script src="https://cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv.min.js" integrity="sha256-3Jy/GbSLrg0o9y5Z5n1uw0qxZECH7C6OQpVBgNFYa0g=" crossorigin="anonymous"></script>
    	<script src="https://cdnjs.cloudflare.com/ajax/libs/respond.js/1.4.2/respond.min.js" integrity="sha256-g6iAfvZp+nDQ2TdTR/VVKJf3bGro4ub5fvWSWVRi2NE=" crossorigin="anonymous"></script>
		<script src="https://cdnjs.cloudflare.com/ajax/libs/es5-shim/4.5.9/es5-shim.min.js" integrity="sha256-8E4Is26QH0bD52WoQpcB+R/tcWQtpzlCojrybUd7Mxo=" crossorigin="anonymous"></script>
    <![endif]-->
</head>

<body>
    <div id="doc" class="markdown-body container-fluid comment-enabled comment-inner" data-hard-breaks="true"><h2 id="Contents" data-id="Contents"><a class="anchor hidden-xs" href="#Contents" title="Contents"><span class="octicon octicon-link"></span></a><span>Contents</span></h2><ol>
<li><a href="#introduction-to-word-segmentation"><span>Introduction to Word Segmentation</span></a></li>
<li><a href="#infant-learning"><span>Infant Learning</span></a></li>
<li><a href="#neural-word-segmentation"><span>Neural Word Segmentation</span></a></li>
<li><a href="#probabilistic-modeling"><span>Probabilistic Modeling</span></a></li>
<li><a href="#references"><span>References</span></a></li>
</ol><h2 id="Introduction-to-Word-Segmentation" data-id="Introduction-to-Word-Segmentation"><a class="anchor hidden-xs" href="#Introduction-to-Word-Segmentation" title="Introduction-to-Word-Segmentation"><span class="octicon octicon-link"></span></a><span>Introduction to Word Segmentation</span></h2><p><span>Word segmentation is the process of determining the word boundaries in free-flowing speech or non-segmented text. Language learners of all ages are able to naturally demarcate word boundaries from continuous speech, even without appreciable pauses or other linguistic cues (as mentioned in Saffran et al. 1996). So how is that human cognition allows for word segmentation within such poverty of stimulus? And is there a way we can capture the same computationally for our use in language modeling and beyond?</span></p><p><span>There have been several attempts and ongoing work for finding different algorithms that will efficiently and accurately segment any given sentence, text, or speech, either in English or in another language. One of the pioneers in this was Saffran et al., who analyzed how children performed this task from a very young age. After her study, different approaches were taken, both in the classical statistics (Brent 1999, Venkataraman 2017), Bayesian realms (Goldwater et al. 2009), and neural learning (Yang et al. 2017, Zheng et al. 2013, Pei et al. 2014, Morita et al. 2015, Chen et al. 2015, Cai and Zhao 2016, Zhang et al. 2016). In this blog, we explore neural learning methods for word segmentation, particularly Yang et al’s 2017 paper, and then contrast them with probabilistic modeling. We develop on and implement some probabilistic models, and analyze their performance on the segmentation task, given different unsegmented corpora, especially in relation with human judgments.</span></p><h3 id="Motivation" data-id="Motivation"><a class="anchor hidden-xs" href="#Motivation" title="Motivation"><span class="octicon octicon-link"></span></a><span>Motivation</span></h3><p><span>We believe that current state-of-the-art language models, which fail on natural language understanding and inference tasks, could benefit from human-inspired augmentations and that an improved word segmentation algorithm would further the current NLP frontiers in the capabilities of neural and non-neural language models, especially because most models currently in place crucially rely on segmenting words correctly. Distilling the knowledge gained from our understanding of human cognition into computational models and human-like intelligent systems could go a long way in overall improved neural learning models.</span></p><h2 id="Infant-Learning" data-id="Infant-Learning"><a class="anchor hidden-xs" href="#Infant-Learning" title="Infant-Learning"><span class="octicon octicon-link"></span></a><span>Infant Learning</span></h2><p><span>Saffran et al.'s groundbreaking paper delves into statistical learning by 8-month-old infants and aims to probe one of the very basic human cognitive tasks, a fundamental task accomplished by almost every child in the world—segmentation of words from fluent speech. The authors state that ‘successful’ word segmentation by the infants, based on only 2 minutes of speech exposure, suggests that they have access to a powerful mechanism for computing the statistical properties of language input. This is a very important observation in building computational models of cognition regarding word segmentation, especially when coupled with the fact that there exists complex and widely varying acoustic structures of speech in different languages and hence, there is no invariant acoustic cue to word boundaries present in all languages.</span></p><p><span>Saffran et al.'s observations are crucial as we set out to use knowledge of how human intelligence works in order to build more human-like intelligence systems. Neural network models have been exploited due to their strength in non-sparse representation learning and non-linear power in feature combination, resulting in multiple neural word segmentors with comparable accuracies to statistical models. Character embeddings (Zheng et al. 2013), character bigrams (Mansur et al. 2013 and Pei et al. 2014), and words (Morita et al. 2015 and Zhang et al. 2016) leverage non-sparse representations to improve segmentation. On the other hand, non-linear modeling power has been leveraged in multiple neural learning based word segmentation paper—such as multi-layer perceptrons in Zheng et al. 2013, Mansur et al. 2013, Pei et al. 2014, Chen et al. 2015; LSTMs on characters in Chen et al. 2015, Xu and Sun 2016; LSTMs on words in Morita et al. 2015, Cai and Zhao 2015, Zhang et al. 2016. All these models leverage salient features of deep neural networks in word segmentation. On the other hand, as outlined in Goodman et al. 2014, Piantadosi et al. 2012, Piantadosi et al. 2016, and multiple other papers, the probabilistic language of thought hypothesis believe that concepts have a language-like compositionality and encode probabilistic knowledge, thereupon relying on Bayesian inference for production. We also look at how Goldwater et al. 2009 approach the word segmentation problem probabilistically, relying on word context. Finally, we try to revisit Saffran et al.'s experiment, this time computationally.</span></p><h2 id="Neural-Word-Segmentation" data-id="Neural-Word-Segmentation"><a class="anchor hidden-xs" href="#Neural-Word-Segmentation" title="Neural-Word-Segmentation"><span class="octicon octicon-link"></span></a><span>Neural Word Segmentation</span></h2><p><span>Word segmentation literature has fairly recently shifted its focus from the statistical methods mentioned above to deep learning technologies. Various salient aspects of deep learning methods have been exploited in the context of word representation, such as using character embeddings as a foundation of neural word segmentors to reduce the sparsity of character n-grams. The non-linear modeling power of neural network structures has been leveraged to represent contexts for segmentation disambiguation. NLP tasks ranging from parsing to sentence generation have uniformly benefited from pre-training, the human-inspired concept of training a model with one task to help it form parameters that can be used in other tasks. The old knowledge helps new models successfully perform new tasks from experience instead of from scratch. However, there existed a gap in the literature on using pretraining for word segmentation tasks, which Yan et al’s 2017 paper aims to fulfill. Their model is conceptually simple and modular, so that the most important sub module, namely a five-character window context, can be pretrained using external data. The authors adopt a multi-task learning strategy (Collobert et al., 2011), casting each external source of information as an auxiliary classification task, sharing a five-character window network. After pretraining, the character window network is used to initialize the corresponding module in the word segmentor. This method outperforms the best statistical and neural segmentation models consistently, giving the best reported results on 5 datasets in different domains and genres.</span></p><h3 id="Model-Structure" data-id="Model-Structure"><a class="anchor hidden-xs" href="#Model-Structure" title="Model-Structure"><span class="octicon octicon-link"></span></a><span>Model Structure</span></h3><p><span>The word segmentor works incrementally from left to right, as the example shown in Table 1. At each step, the state consists of a sequence of words that have been fully recognized, denoted as </span><span class="mathjax raw"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-287"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-288">W</span><span class="MJXp-mo" id="MJXp-Span-289" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-mo" id="MJXp-Span-290" style="margin-left: 0em; margin-right: 0em;">[</span><span class="MJXp-msubsup" id="MJXp-Span-291"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-292" style="margin-right: 0.05em;">w</span><span class="MJXp-mrow MJXp-script" id="MJXp-Span-293" style="vertical-align: -0.4em;"><span class="MJXp-mo" id="MJXp-Span-294">−</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-295">k</span></span></span><span class="MJXp-mo" id="MJXp-Span-296" style="margin-left: 0em; margin-right: 0.222em;">,</span><span class="MJXp-msubsup" id="MJXp-Span-297"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-298" style="margin-right: 0.05em;">w</span><span class="MJXp-mrow MJXp-script" id="MJXp-Span-299" style="vertical-align: -0.4em;"><span class="MJXp-mo" id="MJXp-Span-300">−</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-301">k</span><span class="MJXp-mo" id="MJXp-Span-302">+</span><span class="MJXp-mn" id="MJXp-Span-303">1</span></span></span><span class="MJXp-mo" id="MJXp-Span-304" style="margin-left: 0em; margin-right: 0.222em;">,</span><span class="MJXp-mo" id="MJXp-Span-305" style="margin-left: 0em; margin-right: 0em;">…</span><span class="MJXp-mo" id="MJXp-Span-306" style="margin-left: 0em; margin-right: 0.222em;">,</span><span class="MJXp-msubsup" id="MJXp-Span-307"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-308" style="margin-right: 0.05em;">w</span><span class="MJXp-mrow MJXp-script" id="MJXp-Span-309" style="vertical-align: -0.4em;"><span class="MJXp-mo" id="MJXp-Span-310">−</span><span class="MJXp-mn" id="MJXp-Span-311">1</span></span></span><span class="MJXp-mo" id="MJXp-Span-312" style="margin-left: 0em; margin-right: 0em;">]</span></span></span><script type="math/tex" id="MathJax-Element-46">W = [w_{-k}, w_{-k+1}, \dots, w_{-1}]</script></span><span>, a current partially recognized word </span><span class="mathjax raw"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-313"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-314">P</span></span></span><script type="math/tex" id="MathJax-Element-47">P</script></span><span>, and a sequence of next incoming characters, denoted as </span><span class="mathjax raw"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-315"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-316">C</span><span class="MJXp-mo" id="MJXp-Span-317" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-mo" id="MJXp-Span-318" style="margin-left: 0em; margin-right: 0em;">[</span><span class="MJXp-msubsup" id="MJXp-Span-319"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-320" style="margin-right: 0.05em;">c</span><span class="MJXp-mn MJXp-script" id="MJXp-Span-321" style="vertical-align: -0.4em;">0</span></span><span class="MJXp-mo" id="MJXp-Span-322" style="margin-left: 0em; margin-right: 0.222em;">,</span><span class="MJXp-msubsup" id="MJXp-Span-323"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-324" style="margin-right: 0.05em;">c</span><span class="MJXp-mn MJXp-script" id="MJXp-Span-325" style="vertical-align: -0.4em;">1</span></span><span class="MJXp-mo" id="MJXp-Span-326" style="margin-left: 0em; margin-right: 0.222em;">,</span><span class="MJXp-mo" id="MJXp-Span-327" style="margin-left: 0em; margin-right: 0em;">…</span><span class="MJXp-mo" id="MJXp-Span-328" style="margin-left: 0em; margin-right: 0.222em;">,</span><span class="MJXp-msubsup" id="MJXp-Span-329"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-330" style="margin-right: 0.05em;">c</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-331" style="vertical-align: -0.4em;">m</span></span><span class="MJXp-mo" id="MJXp-Span-332" style="margin-left: 0em; margin-right: 0em;">]</span></span></span><script type="math/tex" id="MathJax-Element-48">C = [c_0, c_1, \dots, c_m]</script></span><span>, as shown in Figure 1 below.</span></p><p><a href="https://imgur.com/81XupE2" target="_blank" rel="noopener"><img src="https://i.imgur.com/81XupE2.png" title="source: imgur.com"></a></p><p><a href="https://imgur.com/4lstKWR" target="_blank" rel="noopener"><img src="https://i.imgur.com/4lstKWR.png" title="source: imgur.com"></a></p><p><span>The model starts by initializing </span><span class="mathjax raw"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-333"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-334">W</span></span></span><script type="math/tex" id="MathJax-Element-49">W</script></span><span> and </span><span class="mathjax raw"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-335"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-336">P</span></span></span><script type="math/tex" id="MathJax-Element-50">P</script></span><span> as </span><span class="mathjax raw"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-337"><span class="MJXp-mo" id="MJXp-Span-338" style="margin-left: 0em; margin-right: 0em;">[</span><span class="MJXp-mo" id="MJXp-Span-339" style="margin-left: 0em; margin-right: 0em;">]</span></span></span><script type="math/tex" id="MathJax-Element-51">[]</script></span><span> and </span><span class="mathjax raw"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-340"><span class="MJXp-mi" id="MJXp-Span-341">Φ</span></span></span><script type="math/tex" id="MathJax-Element-52">\Phi</script></span><span>, and </span><span class="mathjax raw"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-342"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-343">C</span></span></span><script type="math/tex" id="MathJax-Element-53">C</script></span><span> with all the input characters. At each step, the model makes a decision on </span><span class="mathjax raw"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-344"><span class="MJXp-msubsup" id="MJXp-Span-345"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-346" style="margin-right: 0.05em;">c</span><span class="MJXp-mn MJXp-script" id="MJXp-Span-347" style="vertical-align: -0.4em;">0</span></span></span></span><script type="math/tex" id="MathJax-Element-54">c_0</script></span><span>, either deciding to add it as a part of the current word </span><span class="mathjax raw"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-348"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-349">P</span></span></span><script type="math/tex" id="MathJax-Element-55">P</script></span><span>, or demarcating it as the beginning of a new word. This incremental process repeats until </span><span class="mathjax raw"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-350"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-351">C</span></span></span><script type="math/tex" id="MathJax-Element-56">C</script></span><span>, the character bank, is empty, and </span><span class="mathjax raw"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-352"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-353">P</span></span></span><script type="math/tex" id="MathJax-Element-57">P</script></span><span> is null again. This process can be formally represented as a state-transition process, where a state is a tuple </span><span class="mathjax raw"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-354"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-355">S</span><span class="MJXp-mo" id="MJXp-Span-356" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-mo" id="MJXp-Span-357" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-358">W</span><span class="MJXp-mo" id="MJXp-Span-359" style="margin-left: 0em; margin-right: 0.222em;">,</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-360">P</span><span class="MJXp-mo" id="MJXp-Span-361" style="margin-left: 0em; margin-right: 0.222em;">,</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-362">C</span><span class="MJXp-mo" id="MJXp-Span-363" style="margin-left: 0em; margin-right: 0em;">)</span></span></span><script type="math/tex" id="MathJax-Element-58">S = (W, P, C)</script></span><span> and the transition actions include </span><code>Sep</code><span> (separate) and </span><code>App</code><span> (append), as is demonstrated in the deduction system in Figure 2. An end-of-sentence marker is also used. In the figure, </span><span class="mathjax raw"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-364"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-365">V</span></span></span><script type="math/tex" id="MathJax-Element-59">V</script></span><span> denotes the score of a state, given by a neural network model. The score of the initial state is 0, and the score of other states is the sum of scores of all incremental decisions resulting in the state. The overall score is used for disambiguating states, which correspond to sequences of inter-dependent transition actions.</span></p><p><a href="https://imgur.com/NURdJqK" target="_blank" rel="noopener"><img src="https://i.imgur.com/NURdJqK.png" title="source: imgur.com"></a></p><h3 id="Scoring-Network" data-id="Scoring-Network"><a class="anchor hidden-xs" href="#Scoring-Network" title="Scoring-Network"><span class="octicon octicon-link"></span></a><span>Scoring Network</span></h3><p><span>The scoring network used in this model consists of three main layers. The bottommost layer is a representation layer, deriving dense representations </span><span class="mathjax raw"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-366"><span class="MJXp-msubsup" id="MJXp-Span-367"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-368" style="margin-right: 0.05em;">X</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-369" style="vertical-align: -0.4em;">W</span></span><span class="MJXp-mo" id="MJXp-Span-370" style="margin-left: 0em; margin-right: 0.222em;">,</span><span class="MJXp-msubsup" id="MJXp-Span-371"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-372" style="margin-right: 0.05em;">X</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-373" style="vertical-align: -0.4em;">P</span></span><span class="MJXp-mo" id="MJXp-Span-374" style="margin-left: 0em; margin-right: 0.222em;">,</span><span class="MJXp-msubsup" id="MJXp-Span-375"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-376" style="margin-right: 0.05em;">X</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-377" style="vertical-align: -0.4em;">C</span></span></span></span><script type="math/tex" id="MathJax-Element-60">X_W, X_P, X_C</script></span><span> for </span><span class="mathjax raw"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-378"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-379">W</span><span class="MJXp-mo" id="MJXp-Span-380" style="margin-left: 0em; margin-right: 0.222em;">,</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-381">P</span></span></span><script type="math/tex" id="MathJax-Element-61">W, P</script></span><span> and </span><span class="mathjax raw"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-382"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-383">C</span></span></span><script type="math/tex" id="MathJax-Element-62">C</script></span><span>, respectively. The next layer is a hidden layer, used to merge </span><span class="mathjax raw"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-384"><span class="MJXp-msubsup" id="MJXp-Span-385"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-386" style="margin-right: 0.05em;">X</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-387" style="vertical-align: -0.4em;">W</span></span><span class="MJXp-mo" id="MJXp-Span-388" style="margin-left: 0em; margin-right: 0.222em;">,</span><span class="MJXp-msubsup" id="MJXp-Span-389"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-390" style="margin-right: 0.05em;">X</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-391" style="vertical-align: -0.4em;">P</span></span><span class="MJXp-mo" id="MJXp-Span-392" style="margin-left: 0em; margin-right: 0.222em;">,</span><span class="MJXp-msubsup" id="MJXp-Span-393"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-394" style="margin-right: 0.05em;">X</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-395" style="vertical-align: -0.4em;">C</span></span></span></span><script type="math/tex" id="MathJax-Element-63">X_W, X_P, X_C</script></span><span> into a single vector using </span><span class="mathjax raw"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-396"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-397">h</span><span class="MJXp-mo" id="MJXp-Span-398" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-399">t</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-400">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-401">n</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-402">h</span><span class="MJXp-mo" id="MJXp-Span-403" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-msubsup" id="MJXp-Span-404"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-405" style="margin-right: 0.05em;">W</span><span class="MJXp-mrow MJXp-script" id="MJXp-Span-406" style="vertical-align: -0.4em;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-407">h</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-408">W</span></span></span><span class="MJXp-mo" id="MJXp-Span-409" style="margin-left: 0.267em; margin-right: 0.267em;">⋅</span><span class="MJXp-msubsup" id="MJXp-Span-410"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-411" style="margin-right: 0.05em;">X</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-412" style="vertical-align: -0.4em;">W</span></span><span class="MJXp-mo" id="MJXp-Span-413" style="margin-left: 0.267em; margin-right: 0.267em;">+</span><span class="MJXp-msubsup" id="MJXp-Span-414"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-415" style="margin-right: 0.05em;">W</span><span class="MJXp-mrow MJXp-script" id="MJXp-Span-416" style="vertical-align: -0.4em;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-417">h</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-418">P</span></span></span><span class="MJXp-mo" id="MJXp-Span-419" style="margin-left: 0.267em; margin-right: 0.267em;">⋅</span><span class="MJXp-msubsup" id="MJXp-Span-420"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-421" style="margin-right: 0.05em;">X</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-422" style="vertical-align: -0.4em;">P</span></span><span class="MJXp-mo" id="MJXp-Span-423" style="margin-left: 0.267em; margin-right: 0.267em;">+</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-424">W</span><span class="MJXp-mrow" id="MJXp-Span-425"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-426">h</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-427">C</span></span><span class="MJXp-mo" id="MJXp-Span-428" style="margin-left: 0.267em; margin-right: 0.267em;">⋅</span><span class="MJXp-msubsup" id="MJXp-Span-429"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-430" style="margin-right: 0.05em;">X</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-431" style="vertical-align: -0.4em;">c</span></span><span class="MJXp-mo" id="MJXp-Span-432" style="margin-left: 0.267em; margin-right: 0.267em;">+</span><span class="MJXp-msubsup" id="MJXp-Span-433"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-434" style="margin-right: 0.05em;">b</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-435" style="vertical-align: -0.4em;">h</span></span><span class="MJXp-mo" id="MJXp-Span-436" style="margin-left: 0em; margin-right: 0em;">)</span></span></span><script type="math/tex" id="MathJax-Element-64">h = tanh(W_{hW} \cdot X_W + W_{hP} \cdot X_P + W{hC} \cdot X_c + b_h)</script></span><span>. This hidden feature </span><span class="mathjax raw"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-437"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-438">h</span></span></span><script type="math/tex" id="MathJax-Element-65">h</script></span><span> is used to represent the next state </span><span class="mathjax raw"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-439"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-440">S</span><span class="MJXp-mo" id="MJXp-Span-441" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-mo" id="MJXp-Span-442" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-443">W</span><span class="MJXp-mo" id="MJXp-Span-444" style="margin-left: 0em; margin-right: 0.222em;">,</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-445">P</span><span class="MJXp-mo" id="MJXp-Span-446" style="margin-left: 0em; margin-right: 0.222em;">,</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-447">C</span><span class="MJXp-mo" id="MJXp-Span-448" style="margin-left: 0em; margin-right: 0em;">)</span></span></span><script type="math/tex" id="MathJax-Element-66">S=(W,P,C)</script></span><span> for calculating the scores for the next action. A linear output layer with two nodes is used for this with </span><span class="mathjax raw"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-449"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-450">o</span><span class="MJXp-mo" id="MJXp-Span-451" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-msubsup" id="MJXp-Span-452"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-453" style="margin-right: 0.05em;">W</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-454" style="vertical-align: -0.4em;">o</span></span><span class="MJXp-mo" id="MJXp-Span-455" style="margin-left: 0.267em; margin-right: 0.267em;">⋅</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-456">h</span><span class="MJXp-mo" id="MJXp-Span-457" style="margin-left: 0.267em; margin-right: 0.267em;">+</span><span class="MJXp-msubsup" id="MJXp-Span-458"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-459" style="margin-right: 0.05em;">b</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-460" style="vertical-align: -0.4em;">o</span></span></span></span><script type="math/tex" id="MathJax-Element-67">o=W_o\cdot h+b_o</script></span><span>. The two nodes of </span><span class="mathjax raw"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-461"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-462">o</span></span></span><script type="math/tex" id="MathJax-Element-68">o</script></span><span> represent the scores of </span><code>Sep</code><span> and </span><code>App</code><span>, given </span><span class="mathjax raw"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-463"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-464">S</span></span></span><script type="math/tex" id="MathJax-Element-69">S</script></span><span>, respectively.</span></p><h3 id="Pretraining" data-id="Pretraining"><a class="anchor hidden-xs" href="#Pretraining" title="Pretraining"><span class="octicon octicon-link"></span></a><span>Pretraining</span></h3><p><span>The three basic elements in the neural word segmentor, namely characters, character bigrams, and words, can all be pretrained over large unsegmented data. The authors pretrain the five character window network in Figure 3 as a unit, learning the MLP parameter together with character and bigram embeddings. They consider four types of commonly explored external data, all of which have been studied for statistical word segmentation, but not for neural network segmentors.</span></p><p><a href="https://imgur.com/35Z7n4R" target="_blank" rel="noopener"><img src="https://i.imgur.com/35Z7n4R.png" title="source: imgur.com"></a></p><h3 id="Decoding-and-Training" data-id="Decoding-and-Training"><a class="anchor hidden-xs" href="#Decoding-and-Training" title="Decoding-and-Training"><span class="octicon octicon-link"></span></a><span>Decoding and Training</span></h3><p><span>To train the main segmentor, the authors adopt the global transition-based learning and beam-search strategy of Zhang and Clark (2011), as shown in Algorithm 1.</span></p><p><a href="https://imgur.com/J3vT7hF" target="_blank" rel="noopener"><img src="https://i.imgur.com/J3vT7hF.png" title="source: imgur.com"></a></p><p><span>For decoding, a standard beam search is used, where the </span><span class="mathjax raw"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-465"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-466">B</span></span></span><script type="math/tex" id="MathJax-Element-70">B</script></span><span> best partial output hypotheses at each step are maintained in an agenda initialized with the start state. At each step, all hypotheses in the agenda are explored, by applying all the possible actions and the </span><span class="mathjax raw"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-467"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-468">B</span></span></span><script type="math/tex" id="MathJax-Element-71">B</script></span><span> highest scoring hypotheses are used as the agenda for the next step. For training, the same decoding process is applied to each training example </span><span class="mathjax raw"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-469"><span class="MJXp-mo" id="MJXp-Span-470" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-msubsup" id="MJXp-Span-471"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-472" style="margin-right: 0.05em;">x</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-473" style="vertical-align: 0.5em;">i</span></span><span class="MJXp-mo" id="MJXp-Span-474" style="margin-left: 0em; margin-right: 0.222em;">,</span><span class="MJXp-msubsup" id="MJXp-Span-475"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-476" style="margin-right: 0.05em;">y</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-477" style="vertical-align: 0.5em;">i</span></span><span class="MJXp-mo" id="MJXp-Span-478" style="margin-left: 0em; margin-right: 0em;">)</span></span></span><script type="math/tex" id="MathJax-Element-72">(x^i, y^i)</script></span><span>, along with an early update strategy. The authors use Adagrad to optimize the model parameters, and use L2 regularization and dropout to reduce overfitting. Character and character bigram embeddings are fine-tuned, but not word embeddings, according to Zhang et al.'s 2016 conclusion. The hyper-parameter values are shown in Table 2.</span></p><p><a href="https://imgur.com/irhdUAg" target="_blank" rel="noopener"><img src="https://i.imgur.com/irhdUAg.png" title="source: imgur.com"></a></p><h3 id="Experiments" data-id="Experiments"><a class="anchor hidden-xs" href="#Experiments" title="Experiments"><span class="octicon octicon-link"></span></a><span>Experiments</span></h3><p><span>The authors use Chinese Treebank 6.0 (CTB6) (Xue et al., 2005) as their main dataset, along with the SIGHAN 2005 bake-off (Emerson, 2005) and NLPCC 2016 shared task for Weibo segmentation (Qiu et al., 2016). Chinese gigaword is used for pretraining embedding and it is automatically segmented using ZPar 0.6 (Zhang and Clark, 2007). Standard word precision, recall, and F1 are used as evaluation metrics. The authors also perform development experiments to verify the usefulness of various context representations (by varying </span><span class="mathjax raw"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-479"><span class="MJXp-msubsup" id="MJXp-Span-480"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-481" style="margin-right: 0.05em;">X</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-482" style="vertical-align: -0.4em;">C</span></span></span></span><script type="math/tex" id="MathJax-Element-73">X_C</script></span><span> and </span><span class="mathjax raw"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-483"><span class="MJXp-msubsup" id="MJXp-Span-484"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-485" style="margin-right: 0.05em;">X</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-486" style="vertical-align: -0.4em;">W</span></span></span></span><script type="math/tex" id="MathJax-Element-74">X_W</script></span><span>, network configurations (by measuring the influence of beam size on the baseline segmentor) and different pretraining methods (using punctuation methods, etc.) The F1 measures are in Tables 4 and 5 below.</span></p><p><a href="https://imgur.com/k2aQeqV" target="_blank" rel="noopener"><img src="https://i.imgur.com/k2aQeqV.png" title="source: imgur.com"></a></p><p><span>The final results are as in Tables 7 and 8. The final results compare favorably to existing statistical models, and this is the first time a pure neural network model outperforms all existing methods on this dataset, showcasing the helpfulness of pretraining and deep learning methodologies, as well as being a pioneering work in the area. This model also outperforms the best existing neural models, including a hybrid of a statistical and neural model as in Zhang et al. 2016b.</span></p><p><a href="https://imgur.com/dyc1a6G" target="_blank" rel="noopener"><img src="https://i.imgur.com/dyc1a6G.png" title="source: imgur.com"></a></p><p><a href="https://imgur.com/QIlnZDZ" target="_blank" rel="noopener"><img src="https://i.imgur.com/QIlnZDZ.png" title="source: imgur.com"></a></p><h3 id="Conclusion" data-id="Conclusion"><a class="anchor hidden-xs" href="#Conclusion" title="Conclusion"><span class="octicon octicon-link"></span></a><span>Conclusion</span></h3><p><span>The authors have leveraged rich external resources to pretrain a deep learning model, thereby yielding a more enhanced word segmentation model which utilizes both character and word contexts. The authors use neural multi-task learning to pre-train a set of shared parameters for character contexts. The results indicate a 15.4% relative error reduction and are highly competitive to both statistical and neural state-of-the-art models on six different benchmarks.</span></p><h2 id="Probabilistic-Modeling" data-id="Probabilistic-Modeling"><a class="anchor hidden-xs" href="#Probabilistic-Modeling" title="Probabilistic-Modeling"><span class="octicon octicon-link"></span></a><span>Probabilistic Modeling</span></h2><p><span>To help demonstrate the advantages of this new pretrained deep learning model, we have implemented two statistical methods applied on data obtained from Saffran et al.'s classic 1996 paper. We show how these statistical models, a probabilistic context free grammar, and an n-gram model underperform the neural learning method presented above.</span></p><h3 id="Probabilistic-Context-Free-Grammar" data-id="Probabilistic-Context-Free-Grammar"><a class="anchor hidden-xs" href="#Probabilistic-Context-Free-Grammar" title="Probabilistic-Context-Free-Grammar"><span class="octicon octicon-link"></span></a><span>Probabilistic Context Free Grammar</span></h3><p><span>We modify the usual PCFG setup as follows. Instead of having an input sentence, we have an input speech stream, segmented into syllables. We assume that the smallest part of speech that infants can discern without external knowledge is syllables (Saffran et al. also take tri-syllabic words and look at probability transitions between word boundaries in infants), and our concern is how they break this syllable stream into words. We then segment the speech stream aka ‘Sentence’ into words, which further break into more words or a single word, which break into syllable(s). Our PCFG thus looks as below.</span></p><pre><code class="python hljs"><div class="wrapper"><div class="gutter linenumber"><span></span>
<span></span>
<span></span>
<span></span>
<span></span>
<span></span>
<span></span>
<span></span>
<span></span>
<span></span>
<span></span>
<span></span>
<span></span>
<span></span>
<span></span>
<span></span></div><div class="code"><span class="hljs-string">""" 
Sentence -&gt; Words [1.0]
Words -&gt; Word Words [0.8] | Word [0.2]
Word -&gt; Syllables [1.0]
Syllables -&gt; Syllable Syllables [0.8] | Syllable [0.2]
Syllable -&gt; 'tu' [0.083]
Syllable -&gt; 'pi' [0.168]
Syllable -&gt; 'ro' [0.083]
Syllable -&gt; 'go' [0.083]
Syllable -&gt; 'la' [0.168]
Syllable -&gt; 'bu' [0.083]
Syllable -&gt; 'da' [0.083]
Syllable -&gt; 'ko' [0.083]
Syllable -&gt; 'ti' [0.083]
Syllable -&gt; 'du' [0.083] 
"""</span>
</div></div></code></pre><p><span>Just like Saffran et al., we generate speech stream by randomly concatenating words from the input vocabulary (of 2 minutes = 180 words). The syllable probabilities are then inferred from the speech stream, and the word/syllable break probabilities are a parameter that we tweak and see the results with. We then investigate the various word parses (and corresponding) that these PCFGs give us, as well as the probabilities of those parses. A sample parse tree with high probability is shown below—it shows us how given a stream of syllables, our PCFG breaks down the input </span><code>da ro pi go la tu</code><span> into two words </span><code>daropi</code><span> and </span><code>golatu</code><span>. This is one of the “hard” input examples for the vocabulary consisting of the words </span><code>pigola</code><span>, </span><code>golatu</code><span>, and </span><code>daropi</code><span>, because the ‘part-word’ </span><code>pigola</code><span> spanned the boundary between </span><code>daropi#golatu</code><span>. Below that, we have another parse tree—one with a low probability assigned by the parser, and clearly not adequate. We hypothesize that humans have access to such computing mechanism, and select a high probability parse tree to use in their daily lives.</span></p><p><a href="https://imgur.com/b8swcsj" target="_blank" rel="noopener"><img src="https://i.imgur.com/b8swcsj.png" title="source: imgur.com"></a></p><p><a href="https://imgur.com/EaYRI1E" target="_blank" rel="noopener"><img src="https://i.imgur.com/EaYRI1E.png" title="source: imgur.com"></a></p><p><span>We also try four different parsers and do a comparative analysis of the time taken by them to compute all parses of the input </span><code>da ro pi go la tu</code><span>, in an attempt to compare with human reaction times. We receive the following values, as shown below. Saffran et al. gave the infants a much higher threshold of 2 seconds to judge word familiarity.</span></p><p><a href="https://imgur.com/pvysB0v" target="_blank" rel="noopener"><img src="https://i.imgur.com/pvysB0v.png" title="source: imgur.com"></a></p><h3 id="Dynamic-Programming-with-Probabilistic-n-gram-Modeling" data-id="Dynamic-Programming-with-Probabilistic-n-gram-Modeling"><a class="anchor hidden-xs" href="#Dynamic-Programming-with-Probabilistic-n-gram-Modeling" title="Dynamic-Programming-with-Probabilistic-n-gram-Modeling"><span class="octicon octicon-link"></span></a><span>Dynamic Programming with Probabilistic n-gram Modeling</span></h3><p><span>In addition to the PCFG modeling, we then go on to other probabilistic modeling techniques for word segmentation. We use a model implemented along the lines of Peter Norvig’s in the book ‘Beautiful Data’. We use Norvig’s pre-processed version of the Google Trillion Word Dataset distributed through the Linguistics Data Consortium. This dataset is trimmed of n-grams occurring lower than 40 times, unkified, and sentence demarcations are added. It readily gives us unigram and bigram probabilities, from which we can compute the conditional probabilities as well. A snapshot of the bigram counts data used is below.</span></p><p><a href="https://imgur.com/aikKO8X" target="_blank" rel="noopener"><img src="https://i.imgur.com/aikKO8X.png" title="source: imgur.com"></a></p><p><span>We use two probabilistic models—one based on unigrams and the other on bigrams. We recursively split a stream of text, computing the Naive Bayes probability of the sequence of words thus formed, and use dynamic programming to memoize our computation, preventing us from running into exponential times. The Bayes probability is computed using a probability distribution estimated from the counts in the pre-processed data files, and Laplace additive smoothing is used to estimate the probability of unknown words. We also use surprisal values for the bigram model. Finally, the segmentation with the highest probability, or the lowest surprisal, is chosen as our output segmentation.</span></p><p><span>We create a unit test file, with segmentations of text stream, both straightforward and ambiguous, e.g., ‘choosespain’ can be segmented both as ‘choose spain’ or ‘chooses pain’. If we believe that humans use statistical inference, then we can assume that the former would be more probable than the latter, based on conditional probability counts of the true. This is a hypothesis we test in our model, and it indeed turns out to be true. Overall, while this model performs well, there remain controversies as to how well such models relate to human cognitive processes.</span></p><p><span>While English employs word spacing, which makes word segmentation from written corpora fairly easy, Japanese does not (and neither do Mandarin, Cantonese, and agglutinative languages). To make our model more comparable to the neural model for Chinese shown above, We trained a bigram model on Wikipedia Japanese data and tested it on Zhang Lang’s corpus. The results are much poorer, hovering around 80% in accuracy metrics, contrasting with the over 96% in Yang et al.'s deep learning model tested on Chinese datasets. Overall, we note how deep learning methods have resulted in leaps and bounds of progress in fields typically dominated by statistical methods, and how in particular, the word segmentation task in natural language processing has been performed better by neural methods.</span></p><h2 id="References" data-id="References"><a class="anchor hidden-xs" href="#References" title="References"><span class="octicon octicon-link"></span></a><span>References</span></h2><ol>
<li><span>J. R. Saffran, R. N. Aslin, and E. L. Newport. (1996). Statistical learning by 8-month-old infants. Science, 274(5294):1926 – 1928, 1996.</span></li>
<li><span>J. Yang, Y. Zhang, and F. Dong. (2017). Neural Word Segmentation with Rich Pretraining. Association for Computational Linguistics.</span></li>
<li><span>M. Brent. (1999). An Efficient, Probabilistically Sound Algorithm for Segmentation and Word Discovery. Machine Learning, 34. doi: 10.1023/A:1007541817488.</span></li>
<li><span>S. Goldwater, T. L. Griffiths, and M. Johnson. (2009). A bayesian framework for word segmentation: Exploring the effects of context. Cognition, 112(1):21 – 54. ISSN 0010-0277. doi: 10.1016/j.cognition.2009.03.008.</span></li>
<li><span>N. D. Goodman, J. Tenenbaum, and T. Gerstenberg. (2014). Concepts in a Probabilistic Language of Thought.</span></li>
<li><span>S. T. Piantadosi, J. B. Tenenbaum, and N. D. Goodman. (2012). Bootstrapping in a language of thought: a formal model of numerical concept learning. Cognition, 123(2):199–217. doi: doi/10.1016/j.cognition.2011.11.005.</span></li>
<li><span>A. Venkataraman. (2001). A Statistical Model for Word Discovery in Transcribed Speech. Computational Linguistics, 27(3):352–372, Sept. 2001. ISSN 0891-2017.</span></li>
<li><span>X. Zheng, H. Chen, and T. Xu. (2013). Deep learning for Chinese Word Segmentation and Pos Tagging. In EMNLP. Association for Computational Linguistics, pages 647–657.</span></li>
<li><span>Pei, W., Ge, T., &amp; Chang, B. (2014). Max-Margin Tensor Neural Network for Chinese Word Segmentation. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational Linguistics. doi:doi.org/10.3115/v1/p14-1028</span></li>
<li><span>Morita, H., Kawahara, D., &amp; Kurohashi, S. (2015). Morphological Analysis for Unsegmented Languages using Recurrent Neural Network Language Model. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics. doi:doi.org/10.18653/v1/d15-1276</span></li>
<li><span>Chen, X., Qiu, X., Zhu, C., Liu, P., &amp; Huang, X. (2015). Long Short-Term Memory Neural Networks for Chinese Word Segmentation. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics. doi:doi.org/10.18653/v1/d15-1141</span></li>
<li><span>Cai, D., &amp; Zhao, H. (2016). Neural Word Segmentation Learning for Chinese. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational Linguistics. doi:doi.org/10.18653/v1/p16-1039</span></li>
<li><span>Zhang, M., Zhang, Y., &amp; Fu, G. (2016). Transition-Based Neural Word Segmentation. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational Linguistics. doi:doi.org/10.18653/v1/p16-1040</span></li>
<li><span>Mansur, M., Pei, W., Change, B. (2013). Feature-based Neural Language Model and Chinese Word Segmentation. In Proceedings of the Sixth International Joint Conference on Natural Language Processing. Association for Computational Linguistics.</span></li>
<li><span>Chen, X., Qiu, X., Zhu, C., &amp; Huang, X. (2015). Gated Recursive Neural Network for Chinese Word Segmentation. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). Association for Computational Linguistics. doi:doi.org/10.3115/v1/p15-1168</span></li>
<li><span>Andor, D., Alberti, C., Weiss, D., Severyn, A., Presta, A., Ganchev, K., Petrov, S., &amp; Collins, M. (2016). Globally Normalized Transition-Based Neural Networks. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational Linguistics. doi:doi.org/10.18653/v1/p16-1231</span></li>
<li><span>Xue, N., Xia, F., Chiou, F., and Palmer, M. (2005). The Penn Chinese treebank: Phrase structure annotation of a large corpus. Natural language engineering 11(02):207–238.</span></li>
<li><span>Xue, N. (2003). Chinese word segmentation as character tagging. Computational Linguist.</span></li>
<li><span>Zhang , Y., &amp; Clark, S. (2007). Chinese Segmentation with a Word-Based Perceptron Algorithm. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics. Association for Computational Linguistics.</span></li>
<li><span>Zhang , Y., &amp; Clark, S. (2007). Joint Word Segmentation and POS Tagging Using a Single Perceptron. In Proceedings of ACL-08: HLT. Association for Computational Linguistics.</span></li>
<li><span>Zhang, Y., &amp; Clark, S. (2011). Syntactic Processing Using the Generalized Perceptron and Beam Search. In Computational Linguistics (Vol. 37, Issue 1, pp. 105–151). MIT Press - Journals. doi:doi.org/10.1162/coli_a_00037</span></li>
<li><span>Xia, Q., Li, Z., Chao, J., Zhang, M. (2016). Word segmentation on micro-blog texts with external lexicon and heterogeneous data. In International Conference on Computer Processing of Oriental Languages. Springer</span></li>
</ol></div>
    <div class="ui-toc dropup unselectable hidden-print" style="display:none;">
        <div class="pull-right dropdown">
            <a id="tocLabel" class="ui-toc-label btn btn-default" data-toggle="dropdown" href="#" role="button" aria-haspopup="true" aria-expanded="false" title="Table of content">
                <i class="fa fa-bars"></i>
            </a>
            <ul id="ui-toc" class="ui-toc-dropdown dropdown-menu" aria-labelledby="tocLabel">
                <div class="toc"><ul class="nav">
<li><a href="#Contents" title="Contents">Contents</a></li>
<li><a href="#Introduction-to-Word-Segmentation" title="Introduction to Word Segmentation">Introduction to Word Segmentation</a><ul class="nav">
<li><a href="#Motivation" title="Motivation">Motivation</a></li>
</ul>
</li>
<li><a href="#Infant-Learning" title="Infant Learning">Infant Learning</a></li>
<li><a href="#Neural-Word-Segmentation" title="Neural Word Segmentation">Neural Word Segmentation</a><ul class="nav">
<li><a href="#Model-Structure" title="Model Structure">Model Structure</a></li>
<li><a href="#Scoring-Network" title="Scoring Network">Scoring Network</a></li>
<li><a href="#Pretraining" title="Pretraining">Pretraining</a></li>
<li><a href="#Decoding-and-Training" title="Decoding and Training">Decoding and Training</a></li>
<li><a href="#Experiments" title="Experiments">Experiments</a></li>
<li><a href="#Conclusion" title="Conclusion">Conclusion</a></li>
</ul>
</li>
<li><a href="#Probabilistic-Modeling" title="Probabilistic Modeling">Probabilistic Modeling</a><ul class="nav">
<li><a href="#Probabilistic-Context-Free-Grammar" title="Probabilistic Context Free Grammar">Probabilistic Context Free Grammar</a></li>
<li><a href="#Dynamic-Programming-with-Probabilistic-n-gram-Modeling" title="Dynamic Programming with Probabilistic n-gram Modeling">Dynamic Programming with Probabilistic n-gram Modeling</a></li>
</ul>
</li>
<li><a href="#References" title="References">References</a></li>
</ul>
</div><div class="toc-menu"><a class="expand-toggle" href="#">Expand all</a><a class="back-to-top" href="#">Back to top</a><a class="go-to-bottom" href="#">Go to bottom</a></div>
            </ul>
        </div>
    </div>
    <div id="ui-toc-affix" class="ui-affix-toc ui-toc-dropdown unselectable hidden-print" data-spy="affix" style="top:17px;display:none;" null null>
        <div class="toc"><ul class="nav">
<li><a href="#Contents" title="Contents">Contents</a></li>
<li><a href="#Introduction-to-Word-Segmentation" title="Introduction to Word Segmentation">Introduction to Word Segmentation</a><ul class="nav">
<li><a href="#Motivation" title="Motivation">Motivation</a></li>
</ul>
</li>
<li><a href="#Infant-Learning" title="Infant Learning">Infant Learning</a></li>
<li><a href="#Neural-Word-Segmentation" title="Neural Word Segmentation">Neural Word Segmentation</a><ul class="nav">
<li><a href="#Model-Structure" title="Model Structure">Model Structure</a></li>
<li><a href="#Scoring-Network" title="Scoring Network">Scoring Network</a></li>
<li><a href="#Pretraining" title="Pretraining">Pretraining</a></li>
<li><a href="#Decoding-and-Training" title="Decoding and Training">Decoding and Training</a></li>
<li><a href="#Experiments" title="Experiments">Experiments</a></li>
<li><a href="#Conclusion" title="Conclusion">Conclusion</a></li>
</ul>
</li>
<li><a href="#Probabilistic-Modeling" title="Probabilistic Modeling">Probabilistic Modeling</a><ul class="nav">
<li><a href="#Probabilistic-Context-Free-Grammar" title="Probabilistic Context Free Grammar">Probabilistic Context Free Grammar</a></li>
<li><a href="#Dynamic-Programming-with-Probabilistic-n-gram-Modeling" title="Dynamic Programming with Probabilistic n-gram Modeling">Dynamic Programming with Probabilistic n-gram Modeling</a></li>
</ul>
</li>
<li><a href="#References" title="References">References</a></li>
</ul>
</div><div class="toc-menu"><a class="expand-toggle" href="#">Expand all</a><a class="back-to-top" href="#">Back to top</a><a class="go-to-bottom" href="#">Go to bottom</a></div>
    </div>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.1.1/jquery.min.js" integrity="sha256-hVVnYaiADRTO2PzUGmuLJr8BLUSjGIZsDYGmIJLv2b8=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha256-U5ZEeKfGNOja007MMD3YBI0A3OSZOQbeG6z2f2Y0hu8=" crossorigin="anonymous" defer></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/gist-embed/2.6.0/gist-embed.min.js" integrity="sha256-KyF2D6xPIJUW5sUDSs93vWyZm+1RzIpKCexxElmxl8g=" crossorigin="anonymous" defer></script>
    <script>
        var markdown = $(".markdown-body");
        //smooth all hash trigger scrolling
        function smoothHashScroll() {
            var hashElements = $("a[href^='#']").toArray();
            for (var i = 0; i < hashElements.length; i++) {
                var element = hashElements[i];
                var $element = $(element);
                var hash = element.hash;
                if (hash) {
                    $element.on('click', function (e) {
                        // store hash
                        var hash = this.hash;
                        if ($(hash).length <= 0) return;
                        // prevent default anchor click behavior
                        e.preventDefault();
                        // animate
                        $('body, html').stop(true, true).animate({
                            scrollTop: $(hash).offset().top
                        }, 100, "linear", function () {
                            // when done, add hash to url
                            // (default click behaviour)
                            window.location.hash = hash;
                        });
                    });
                }
            }
        }

        smoothHashScroll();
        var toc = $('.ui-toc');
        var tocAffix = $('.ui-affix-toc');
        var tocDropdown = $('.ui-toc-dropdown');
        //toc
        tocDropdown.click(function (e) {
            e.stopPropagation();
        });

        var enoughForAffixToc = true;

        function generateScrollspy() {
            $(document.body).scrollspy({
                target: ''
            });
            $(document.body).scrollspy('refresh');
            if (enoughForAffixToc) {
                toc.hide();
                tocAffix.show();
            } else {
                tocAffix.hide();
                toc.show();
            }
            $(document.body).scroll();
        }

        function windowResize() {
            //toc right
            var paddingRight = parseFloat(markdown.css('padding-right'));
            var right = ($(window).width() - (markdown.offset().left + markdown.outerWidth() - paddingRight));
            toc.css('right', right + 'px');
            //affix toc left
            var newbool;
            var rightMargin = (markdown.parent().outerWidth() - markdown.outerWidth()) / 2;
            //for ipad or wider device
            if (rightMargin >= 133) {
                newbool = true;
                var affixLeftMargin = (tocAffix.outerWidth() - tocAffix.width()) / 2;
                var left = markdown.offset().left + markdown.outerWidth() - affixLeftMargin;
                tocAffix.css('left', left + 'px');
            } else {
                newbool = false;
            }
            if (newbool != enoughForAffixToc) {
                enoughForAffixToc = newbool;
                generateScrollspy();
            }
        }
        $(window).resize(function () {
            windowResize();
        });
        $(document).ready(function () {
            windowResize();
            generateScrollspy();
        });

        //remove hash
        function removeHash() {
            window.location.hash = '';
        }

        var backtotop = $('.back-to-top');
        var gotobottom = $('.go-to-bottom');

        backtotop.click(function (e) {
            e.preventDefault();
            e.stopPropagation();
            if (scrollToTop)
                scrollToTop();
            removeHash();
        });
        gotobottom.click(function (e) {
            e.preventDefault();
            e.stopPropagation();
            if (scrollToBottom)
                scrollToBottom();
            removeHash();
        });

        var toggle = $('.expand-toggle');
        var tocExpand = false;

        checkExpandToggle();
        toggle.click(function (e) {
            e.preventDefault();
            e.stopPropagation();
            tocExpand = !tocExpand;
            checkExpandToggle();
        })

        function checkExpandToggle () {
            var toc = $('.ui-toc-dropdown .toc');
            var toggle = $('.expand-toggle');
            if (!tocExpand) {
                toc.removeClass('expand');
                toggle.text('Expand all');
            } else {
                toc.addClass('expand');
                toggle.text('Collapse all');
            }
        }

        function scrollToTop() {
            $('body, html').stop(true, true).animate({
                scrollTop: 0
            }, 100, "linear");
        }

        function scrollToBottom() {
            $('body, html').stop(true, true).animate({
                scrollTop: $(document.body)[0].scrollHeight
            }, 100, "linear");
        }
    </script>
</body>

</html>
